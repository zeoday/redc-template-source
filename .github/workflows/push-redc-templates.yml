name: Build and Deploy (Branch Fix)

on:
  push:
    branches: [ "main" ]
  workflow_dispatch:
    inputs:
      force_rebuild:
        description: 'Force rebuild'
        required: false
        default: false
        type: boolean

permissions:
  contents: write  # <--- å¿…é¡»æ”¹æˆ writeï¼Œå¦åˆ™æ— æ³•æŽ¨é€åˆ†æ”¯
  pages: write
  id-token: write

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    steps:
      # 1. æ‹‰å–æºç 
      - name: Checkout Source
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      # 2. æ‹‰å–ç¼“å­˜ (Safe)
      - name: Checkout Previous Build (Safe)
        run: |
          echo "ðŸ” Checking for existing gh-pages branch..."
          if git ls-remote --exit-code --heads origin gh-pages; then
            echo "âœ… Branch found. Cloning cache..."
            git clone --depth=1 --branch=gh-pages https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}.git previous_build
          else
            echo "âš ï¸ Branch 'gh-pages' not found (likely first run). Skipping cache."
            mkdir -p previous_build
          fi

      # 3. æ‰§è¡Œæž„å»º
      - name: Build with Python
        env:
          REPO_OWNER: ${{ github.repository_owner }}
          REPO_NAME: ${{ github.event.repository.name }}
          FORCE_REBUILD: ${{ inputs.force_rebuild }}
        run: |
          mkdir -p public/templates
          [ -f index.html ] && cp index.html public/index.html

          python3 -c "
          import os
          import json
          import shutil
          import hashlib
          import datetime

          repo_owner = os.environ.get('REPO_OWNER')
          repo_name = os.environ.get('REPO_NAME')
          force_rebuild = os.environ.get('FORCE_REBUILD') == 'true'
          
          base_url = f'https://{repo_owner}.github.io/{repo_name}'
          output_dir = 'public'
          templates_dir = os.path.join(output_dir, 'templates')
          prev_build_dir = 'previous_build'
          prev_index_path = os.path.join(prev_build_dir, 'index.json')

          def calculate_dir_hash(directory_path):
              sha256_hash = hashlib.sha256()
              for root, dirs, files in os.walk(directory_path):
                  dirs.sort()
                  for filename in sorted(files):
                      if filename.startswith('.'): continue
                      filepath = os.path.join(root, filename)
                      try:
                          with open(filepath, 'rb') as f:
                              while True:
                                  data = f.read(65536)
                                  if not data: break
                                  sha256_hash.update(data)
                      except: pass
              return sha256_hash.hexdigest()

          old_map = {}
          if os.path.exists(prev_index_path) and not force_rebuild:
              try:
                  with open(prev_index_path, 'r') as f:
                      data = json.load(f)
                      for t in data.get('templates', []):
                          old_map[t['id']] = t
                  print(f'ðŸ“¦ Loaded history for {len(old_map)} templates.')
              except: pass

          new_templates = []
          repo_root = '.'
          exclude = {'.git', '.github', 'public', 'previous_build', 'README.md', '.gitignore', 'index.html'}

          if not os.path.exists(templates_dir): os.makedirs(templates_dir)

          for provider in sorted(os.listdir(repo_root)):
              provider_path = os.path.join(repo_root, provider)
              if not os.path.isdir(provider_path) or provider in exclude: continue

              for template in sorted(os.listdir(provider_path)):
                  template_path = os.path.join(provider_path, template)
                  if not os.path.isdir(template_path): continue

                  template_id = f'{provider}/{template}'
                  zip_name = f'{provider}_{template}.zip'
                  zip_output_path = os.path.join(templates_dir, zip_name)
                  
                  current_src_hash = calculate_dir_hash(template_path)
                  
                  old_data = old_map.get(template_id)
                  prev_zip_path = os.path.join(prev_build_dir, 'templates', zip_name)
                  
                  reason = 'Unknown'
                  should_rebuild = False
                  
                  if force_rebuild:
                      should_rebuild = True; reason = 'Forced'
                  elif not old_data:
                      should_rebuild = True; reason = 'New'
                  elif not os.path.exists(prev_zip_path):
                      should_rebuild = True; reason = 'Missing Artifact'
                  elif old_data.get('source_hash') != current_src_hash:
                      should_rebuild = True; reason = 'Changed'
                  else:
                      reason = 'No Change'

                  final_zip_hash = ''
                  if should_rebuild:
                      print(f'ðŸ”¨ Building: {template_id} [{reason}]')
                      shutil.make_archive(zip_output_path.replace('.zip', ''), 'zip', root_dir=template_path)
                      h = hashlib.sha256()
                      with open(zip_output_path, 'rb') as f:
                          for chunk in iter(lambda: f.read(4096), b''): h.update(chunk)
                      final_zip_hash = h.hexdigest()
                  else:
                      print(f'â© Skipping: {template_id} [Hash Match]')
                      shutil.copy2(prev_zip_path, zip_output_path)
                      final_zip_hash = old_data['sha256']

                  meta_name = template; meta_user = 'Unknown'; meta_desc = 'No description.'
                  try:
                      case_path = os.path.join(template_path, 'case.json')
                      if os.path.exists(case_path):
                          with open(case_path, 'r', encoding='utf-8') as cf:
                              c = json.load(cf)
                              meta_name = c.get('name', template)
                              meta_user = c.get('user', 'Unknown')
                              meta_desc = c.get('DESCRIPTION', 'No description.')
                  except:
                      if old_data:
                          meta_name = old_data['metadata']['name']
                          meta_user = old_data['metadata']['author']
                          meta_desc = old_data['metadata']['description']

                  new_templates.append({
                      'id': template_id, 'provider': provider, 'slug': template,
                      'url': f'{base_url}/templates/{zip_name}',
                      'sha256': final_zip_hash, 'source_hash': current_source_hash,
                      'metadata': {'name': meta_name, 'author': meta_user, 'description': meta_desc}
                  })

          manifest = {'updated_at': datetime.datetime.utcnow().isoformat() + 'Z', 'repo_name': repo_name, 'templates': new_templates}
          with open(os.path.join(output_dir, 'index.json'), 'w', encoding='utf-8') as f:
              json.dump(manifest, f, indent=2, ensure_ascii=False)
          print(f'âœ¨ Build Complete.')
          "

      # 4. éƒ¨ç½² (æ”¹ç”¨ peaceiris æ’ä»¶ï¼Œå¼ºåˆ¶æŽ¨é€åˆ° gh-pages åˆ†æ”¯)
      - name: Deploy to gh-pages
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./public
          publish_branch: gh-pages  # æ˜¾å¼æŒ‡å®šåˆ†æ”¯å
          commit_message: "Deploy: ${{ github.event.head_commit.message }}"
