name: Build and Deploy

on:
  push:
    branches: [ "main" ]
  workflow_dispatch:
    inputs:
      force_rebuild:
        description: 'Force rebuild all templates'
        required: false
        default: 'false'
        type: boolean

permissions:
  contents: read
  pages: write
  id-token: write

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Source
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      # æ‹‰å–ä¸Šæ¬¡æ„å»ºç»“æœä½œä¸ºç¼“å­˜
      - name: Checkout Previous Build
        uses: actions/checkout@v4
        with:
          ref: gh-pages
          path: previous_build
          fetch-depth: 1
        continue-on-error: true

      - name: Smart Incremental Build
        run: |
          mkdir -p public/templates
          cp index.html public/index.html

          python3 -c "
          import os
          import json
          import shutil
          import hashlib
          import datetime

          # --- é…ç½® ---
          repo_owner = '${{ github.repository_owner }}'
          repo_name = '${{ github.event.repository.name }}'
          base_url = f'https://{repo_owner}.github.io/{repo_name}'
          
          output_dir = 'public'
          templates_dir = os.path.join(output_dir, 'templates')
          prev_build_dir = 'previous_build'
          prev_index_path = os.path.join(prev_build_dir, 'index.json')
          
          force_rebuild = '${{ inputs.force_rebuild }}' == 'true'

          # --- æ ¸å¿ƒå‡½æ•°: è®¡ç®—ç›®å½•å†…å®¹çš„å“ˆå¸Œ (æŒ‡çº¹) ---
          def calculate_dir_hash(directory_path):
              sha256_hash = hashlib.sha256()
              # os.walk é»˜è®¤é¡ºåºä¸å›ºå®šï¼Œå¿…é¡» sorted ä¿è¯è·¨å¹³å°/è·¨æ¬¡æ‰§è¡Œçš„ä¸€è‡´æ€§
              for root, dirs, files in os.walk(directory_path):
                  dirs.sort()  # ä¿è¯éå†ç›®å½•é¡ºåº
                  for filename in sorted(files):
                      # æ’é™¤ case.json å˜åŠ¨æ˜¯å¦å½±å“æ‰“åŒ…? é€šå¸¸ case.json å˜äº†ä¹Ÿè¦æ›´æ–° metadata
                      # å¦‚æœæƒ³æ’é™¤æŸäº›æ–‡ä»¶ä¸å‚ä¸æŒ‡çº¹è®¡ç®—ï¼Œå¯ä»¥åœ¨è¿™é‡Œ if continue
                      filepath = os.path.join(root, filename)
                      try:
                          with open(filepath, 'rb') as f:
                              while True:
                                  data = f.read(65536)
                                  if not data: break
                                  sha256_hash.update(data)
                      except: pass
              return sha256_hash.hexdigest()

          # --- 1. åŠ è½½æ—§æ•°æ® ---
          old_map = {}
          if os.path.exists(prev_index_path) and not force_rebuild:
              try:
                  with open(prev_index_path, 'r') as f:
                      data = json.load(f)
                      for t in data.get('templates', []):
                          old_map[t['id']] = t
                  print(f'ğŸ“¦ Loaded history for {len(old_map)} templates.')
              except: pass

          # --- 2. éå†ä¸å¤„ç† ---
          new_templates = []
          repo_root = '.'
          exclude = {'.git', '.github', 'public', 'previous_build', 'README.md', '.gitignore', 'index.html'}

          for provider in sorted(os.listdir(repo_root)):
              provider_path = os.path.join(repo_root, provider)
              if not os.path.isdir(provider_path) or provider in exclude:
                  continue

              for template in sorted(os.listdir(provider_path)):
                  template_path = os.path.join(provider_path, template)
                  if not os.path.isdir(template_path):
                      continue

                  template_id = f'{provider}/{template}'
                  zip_name = f'{provider}_{template}.zip'
                  zip_output_path = os.path.join(templates_dir, zip_name)
                  
                  # A. è®¡ç®—å½“å‰æºç çš„æŒ‡çº¹
                  current_source_hash = calculate_dir_hash(template_path)
                  
                  # B. æ£€æŸ¥æ˜¯å¦éœ€è¦æ„å»º
                  # è§„åˆ™: å¼ºåˆ¶æ„å»º OR æ—§æ•°æ®é‡Œæ²¡æœ‰ OR æŒ‡çº¹å˜äº† OR æ—§ZIPæ–‡ä»¶ä¸¢äº†
                  should_rebuild = False
                  old_data = old_map.get(template_id)
                  
                  # æ£€æŸ¥æ—§ ZIP æ˜¯å¦çœŸçš„å­˜åœ¨ (é˜²æ­¢ç¼“å­˜æŸå)
                  prev_zip_path = os.path.join(prev_build_dir, 'templates', zip_name)
                  prev_zip_exists = os.path.exists(prev_zip_path)

                  if force_rebuild:
                      should_rebuild = True
                      reason = 'Forced'
                  elif not old_data:
                      should_rebuild = True
                      reason = 'New Template'
                  elif not prev_zip_exists:
                      should_rebuild = True
                      reason = 'Missing Artifact'
                  elif old_data.get('source_hash') != current_source_hash:
                      should_rebuild = True
                      reason = 'Content Changed'
                  else:
                      reason = 'No Change'

                  # C. æ‰§è¡Œæ“ä½œ
                  final_zip_hash = ''
                  
                  if should_rebuild:
                      print(f'ğŸ”¨ Building: {template_id} [{reason}]')
                      # æ‰“åŒ…
                      shutil.make_archive(zip_output_path.replace('.zip', ''), 'zip', template_path)
                      
                      # è®¡ç®— ZIP Hash (ç”¨äºä¸‹è½½æ ¡éªŒ)
                      h = hashlib.sha256()
                      with open(zip_output_path, 'rb') as f:
                          for chunk in iter(lambda: f.read(4096), b''):
                              h.update(chunk)
                      final_zip_hash = h.hexdigest()
                      
                  else:
                      print(f'â© Skipping: {template_id} [Hash Match]')
                      # å¤åˆ¶æ—§ ZIP
                      shutil.copy2(prev_zip_path, zip_output_path)
                      # å¤ç”¨æ—§çš„ ZIP Hash
                      final_zip_hash = old_data['sha256']

                  # D. è¯»å–å…ƒæ•°æ® (case.json)
                  # å³ä½¿æ²¡é‡æ–°æ‰“åŒ…ï¼Œä¹Ÿé‡æ–°è¯»ä¸€ä¸‹ case.json æ¯”è¾ƒå¥½å—ï¼Ÿ
                  # ä¸ºäº†æ€§èƒ½ï¼Œå¦‚æœæ²¡é‡å»ºï¼Œç›´æ¥å¤ç”¨æ—§ metadata ä¹Ÿå¯ä»¥ã€‚
                  # ä½†å¦‚æœåªæ”¹äº† case.json çš„æè¿°ï¼ŒæŒ‡çº¹ä¼šå˜ï¼Œè¿™é‡Œå°±ä¼šèµ°è¿› should_rebuildï¼Œæ‰€ä»¥é€»è¾‘æ˜¯è‡ªæ´½çš„ã€‚
                  
                  meta_name = template
                  meta_user = 'Unknown'
                  meta_desc = 'No description.'
                  
                  # å°è¯•è¯»å–æœ¬åœ° case.json (æœ€æ–°çš„)
                  try:
                      with open(os.path.join(template_path, 'case.json'), 'r', encoding='utf-8') as cf:
                          c = json.load(cf)
                          meta_name = c.get('name', template)
                          meta_user = c.get('user', 'Unknown')
                          meta_desc = c.get('DESCRIPTION', 'No description.')
                  except:
                      # å¦‚æœè¯»å–å¤±è´¥ï¼ˆæ¯”å¦‚ JSON æ ¼å¼é”™è¯¯ï¼‰ï¼Œå°è¯•å›é€€åˆ°æ—§æ•°æ®
                      if old_data:
                          meta_name = old_data['metadata']['name']
                          meta_user = old_data['metadata']['author']
                          meta_desc = old_data['metadata']['description']

                  # E. æ·»åŠ åˆ°æ–°åˆ—è¡¨
                  new_templates.append({
                      'id': template_id,
                      'provider': provider,
                      'slug': template,
                      'url': f'{base_url}/templates/{zip_name}',
                      'sha256': final_zip_hash,       # ZIP æ–‡ä»¶çš„ hash
                      'source_hash': current_source_hash, # æºç ç›®å½•çš„æŒ‡çº¹ (å†…éƒ¨é€»è¾‘ç”¨)
                      'metadata': {
                          'name': meta_name,
                          'author': meta_user,
                          'description': meta_desc
                      }
                  })

          # --- 3. ä¿å­˜ ---
          manifest = {
              'updated_at': datetime.datetime.utcnow().isoformat() + 'Z',
              'repo_name': repo_name,
              'templates': new_templates
          }
          
          with open(os.path.join(output_dir, 'index.json'), 'w', encoding='utf-8') as f:
              json.dump(manifest, f, indent=2, ensure_ascii=False)
          
          print('âœ¨ Smart Build Complete.')
          "

      - name: Upload Pages artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: 'public'

      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
